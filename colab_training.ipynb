{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸš Crowd Counting Model Training\n",
                "This notebook trains CSRNet and P2PNet models on the ShanghaiTech dataset using a free GPU.\n",
                "\n",
                "**Instructions:**\n",
                "1. Go to Runtime > Change runtime type > Select **T4 GPU**\n",
                "2. Upload the dataset zip (instructions below)\n",
                "3. Run all cells"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# Check GPU\n",
                "!nvidia-smi\n",
                "import torch\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Mount Google Drive & Upload Dataset\n",
                "Upload your `shanghaitech.zip` to Google Drive first."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Unzip dataset (adjust path if needed)\n",
                "!unzip -q /content/drive/MyDrive/shanghaitech.zip -d /content/data/\n",
                "!ls /content/data/"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Install Dependencies"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install scipy h5py tqdm ultralytics"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: CSRNet Model & Training"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torchvision import models\n",
                "\n",
                "class CSRNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(CSRNet, self).__init__()\n",
                "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
                "        self.frontend = nn.Sequential(*list(vgg.features.children())[:23])\n",
                "        self.backend = nn.Sequential(\n",
                "            nn.Conv2d(512, 512, 3, dilation=2, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, 3, dilation=2, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 512, 3, dilation=2, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(512, 256, 3, dilation=2, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 128, 3, dilation=2, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(128, 64, 3, dilation=2, padding=2),\n",
                "            nn.ReLU(inplace=True),\n",
                "        )\n",
                "        self.output_layer = nn.Conv2d(64, 1, 1)\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.frontend(x)\n",
                "        x = self.backend(x)\n",
                "        x = self.output_layer(x)\n",
                "        return x\n",
                "\n",
                "print(\"CSRNet model defined!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "from PIL import Image\n",
                "import h5py\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms\n",
                "\n",
                "class CrowdDataset(Dataset):\n",
                "    def __init__(self, root_path):\n",
                "        self.root = root_path\n",
                "        self.img_dir = os.path.join(root_path, 'images')\n",
                "        self.gt_dir = os.path.join(root_path, 'ground_truth_h5')\n",
                "        self.images = sorted([f for f in os.listdir(self.img_dir) if f.endswith('.jpg')])\n",
                "        self.transform = transforms.Compose([\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "        ])\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_name = self.images[idx]\n",
                "        img_path = os.path.join(self.img_dir, img_name)\n",
                "        gt_name = img_name.replace('.jpg', '.h5')\n",
                "        gt_path = os.path.join(self.gt_dir, gt_name)\n",
                "        img = Image.open(img_path).convert('RGB')\n",
                "        img = self.transform(img)\n",
                "        with h5py.File(gt_path, 'r') as f:\n",
                "            density = np.array(f['density'])\n",
                "        density = torch.tensor(density, dtype=torch.float32).unsqueeze(0)\n",
                "        return img, density\n",
                "\n",
                "print(\"Dataset class defined!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Generate density maps if not present\n",
                "import scipy.io as sio\n",
                "from scipy.ndimage import gaussian_filter\n",
                "\n",
                "def generate_density_maps(root):\n",
                "    for split in ['train_data', 'test_data']:\n",
                "        split_path = os.path.join(root, split)\n",
                "        img_dir = os.path.join(split_path, 'images')\n",
                "        gt_dir = os.path.join(split_path, 'ground_truth')\n",
                "        h5_dir = os.path.join(split_path, 'ground_truth_h5')\n",
                "        os.makedirs(h5_dir, exist_ok=True)\n",
                "        \n",
                "        images = [f for f in os.listdir(img_dir) if f.endswith('.jpg')]\n",
                "        for img_name in images:\n",
                "            mat_name = 'GT_' + img_name.replace('.jpg', '.mat')\n",
                "            mat_path = os.path.join(gt_dir, mat_name)\n",
                "            h5_path = os.path.join(h5_dir, img_name.replace('.jpg', '.h5'))\n",
                "            \n",
                "            if os.path.exists(h5_path):\n",
                "                continue\n",
                "            \n",
                "            img = Image.open(os.path.join(img_dir, img_name))\n",
                "            w, h = img.size\n",
                "            mat = sio.loadmat(mat_path)\n",
                "            points = mat['image_info'][0][0][0][0][0]\n",
                "            \n",
                "            density = np.zeros((h//8, w//8), dtype=np.float32)\n",
                "            for pt in points:\n",
                "                x, y = int(pt[0]/8), int(pt[1]/8)\n",
                "                if 0 <= x < w//8 and 0 <= y < h//8:\n",
                "                    density[y, x] += 1\n",
                "            density = gaussian_filter(density, sigma=3)\n",
                "            \n",
                "            with h5py.File(h5_path, 'w') as f:\n",
                "                f.create_dataset('density', data=density)\n",
                "        print(f\"Done: {split}\")\n",
                "\n",
                "# Run this (adjust path to your dataset location)\n",
                "generate_density_maps('/content/data/shanghaitech/part_B_final')\n",
                "print(\"Density maps ready!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Train CSRNet\n",
                "from tqdm import tqdm\n",
                "\n",
                "device = torch.device('cuda')\n",
                "print(f\"Training on: {device}\")\n",
                "\n",
                "root = '/content/data/shanghaitech/part_B_final'\n",
                "train_set = CrowdDataset(os.path.join(root, 'train_data'))\n",
                "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
                "val_set = CrowdDataset(os.path.join(root, 'test_data'))\n",
                "val_loader = DataLoader(val_set, batch_size=4, shuffle=False)\n",
                "\n",
                "model = CSRNet().to(device)\n",
                "criterion = nn.MSELoss(reduction='sum').to(device)\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
                "\n",
                "epochs = 50  # Reduced for faster training\n",
                "best_mae = float('inf')\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    model.train()\n",
                "    train_loss = 0.0\n",
                "    for img, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
                "        img, target = img.to(device), target.to(device)\n",
                "        output = model(img)\n",
                "        loss = criterion(output, target)\n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        train_loss += loss.item()\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    mae = 0.0\n",
                "    with torch.no_grad():\n",
                "        for img, target in val_loader:\n",
                "            img, target = img.to(device), target.to(device)\n",
                "            output = model(img)\n",
                "            mae += abs(output.sum().item() - target.sum().item())\n",
                "    mae /= len(val_loader)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} - Loss: {train_loss/len(train_loader):.4f}, MAE: {mae:.2f}\")\n",
                "    \n",
                "    if mae < best_mae:\n",
                "        best_mae = mae\n",
                "        torch.save(model.state_dict(), '/content/drive/MyDrive/csrnet_best.pth')\n",
                "        print(f\"Saved best model (MAE: {mae:.2f})\")\n",
                "\n",
                "print(f\"\\nCSRNet training complete! Best MAE: {best_mae:.2f}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: P2PNet Training"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "# P2PNet Model\n",
                "class P2PNet(nn.Module):\n",
                "    def __init__(self, num_anchors=256):\n",
                "        super(P2PNet, self).__init__()\n",
                "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
                "        self.backbone = nn.Sequential(*list(vgg.features.children())[:23])\n",
                "        self.reg_head = nn.Sequential(\n",
                "            nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(128, 2, 1)\n",
                "        )\n",
                "        self.cls_head = nn.Sequential(\n",
                "            nn.Conv2d(512, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(128, 1, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        features = self.backbone(x)\n",
                "        reg = self.reg_head(features)\n",
                "        cls = self.cls_head(features)\n",
                "        B, _, H, W = reg.shape\n",
                "        reg = reg.permute(0, 2, 3, 1).reshape(B, H*W, 2)\n",
                "        cls = cls.permute(0, 2, 3, 1).reshape(B, H*W)\n",
                "        return reg, cls\n",
                "\n",
                "print(\"P2PNet model defined!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# P2PNet Dataset\n",
                "import scipy.io as sio\n",
                "\n",
                "class P2PDataset(Dataset):\n",
                "    def __init__(self, root_path, transform=None):\n",
                "        self.root = root_path\n",
                "        self.img_dir = os.path.join(root_path, 'images')\n",
                "        self.gt_dir = os.path.join(root_path, 'ground_truth')\n",
                "        self.images = sorted([f for f in os.listdir(self.img_dir) if f.endswith('.jpg')])\n",
                "        self.transform = transform or transforms.Compose([\n",
                "            transforms.ToTensor(),\n",
                "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "        ])\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.images)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_name = self.images[idx]\n",
                "        img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
                "        img = self.transform(img)\n",
                "        mat_name = 'GT_' + img_name.replace('.jpg', '.mat')\n",
                "        mat = sio.loadmat(os.path.join(self.gt_dir, mat_name))\n",
                "        points = mat['image_info'][0][0][0][0][0]\n",
                "        return img, {'points': torch.tensor(points, dtype=torch.float32)}\n",
                "\n",
                "def p2p_collate(batch):\n",
                "    images = torch.stack([b[0] for b in batch])\n",
                "    targets = [b[1] for b in batch]\n",
                "    return images, targets\n",
                "\n",
                "print(\"P2PNet dataset defined!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Train P2PNet (simplified loss)\n",
                "p2p_model = P2PNet().to(device)\n",
                "p2p_optimizer = torch.optim.Adam(p2p_model.parameters(), lr=1e-4)\n",
                "\n",
                "p2p_train = P2PDataset(os.path.join(root, 'train_data'))\n",
                "p2p_train_loader = DataLoader(p2p_train, batch_size=2, shuffle=True, collate_fn=p2p_collate)\n",
                "p2p_val = P2PDataset(os.path.join(root, 'test_data'))\n",
                "p2p_val_loader = DataLoader(p2p_val, batch_size=2, shuffle=False, collate_fn=p2p_collate)\n",
                "\n",
                "best_mae = float('inf')\n",
                "for epoch in range(50):\n",
                "    p2p_model.train()\n",
                "    total_loss = 0\n",
                "    for img, targets in tqdm(p2p_train_loader, desc=f\"P2P Epoch {epoch+1}/50\"):\n",
                "        img = img.to(device)\n",
                "        reg, cls = p2p_model(img)\n",
                "        # Simplified counting loss\n",
                "        pred_counts = torch.sigmoid(cls).sum(dim=1)\n",
                "        gt_counts = torch.tensor([len(t['points']) for t in targets], device=device, dtype=torch.float32)\n",
                "        loss = nn.functional.mse_loss(pred_counts, gt_counts)\n",
                "        p2p_optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        p2p_optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    # Validation\n",
                "    p2p_model.eval()\n",
                "    mae = 0\n",
                "    with torch.no_grad():\n",
                "        for img, targets in p2p_val_loader:\n",
                "            img = img.to(device)\n",
                "            _, cls = p2p_model(img)\n",
                "            pred = (torch.sigmoid(cls) > 0.5).sum(dim=1)\n",
                "            for i, t in enumerate(targets):\n",
                "                mae += abs(pred[i].item() - len(t['points']))\n",
                "    mae /= len(p2p_val)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1} - Loss: {total_loss/len(p2p_train_loader):.4f}, MAE: {mae:.2f}\")\n",
                "    \n",
                "    if mae < best_mae:\n",
                "        best_mae = mae\n",
                "        torch.save(p2p_model.state_dict(), '/content/drive/MyDrive/p2pnet_best.pth')\n",
                "        print(f\"Saved best P2PNet (MAE: {mae:.2f})\")\n",
                "\n",
                "print(f\"\\nP2PNet training complete! Best MAE: {best_mae:.2f}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## âœ… Done!\n",
                "Your trained models are saved to Google Drive:\n",
                "- `csrnet_best.pth`\n",
                "- `p2pnet_best.pth`\n",
                "\n",
                "Download them and copy to your local `models/` folder."
            ],
            "metadata": {}
        }
    ]
}